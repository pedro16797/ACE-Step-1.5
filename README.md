<h1 align="center">ACE-Step 1.5</h1>
<h1 align="center">Pushing the Boundaries of Open-Source Music Generation</h1>
<p align="center">
    <a href="https://ace-step.github.io/ace-step-v1.5.github.io/">Project</a> |
    <a href="https://huggingface.co/ACE-Step/Ace-Step1.5">Hugging Face</a> |
    <a href="https://modelscope.cn/models/ACE-Step/Ace-Step1.5">ModelScope</a> |
    <a href="https://huggingface.co/spaces/ACE-Step/Ace-Step-v1.5">Space Demo</a> |
    <a href="https://discord.gg/PeWDxrkdj7">Discord</a> |
    <a href="https://arxiv.org/abs/2602.00744">Technical Report</a>
</p>

<p align="center">
    <img src="./assets/orgnization_logos.png" width="100%" alt="StepFun Logo">
</p>

## Table of Contents

- [âœ¨ Features](#-features)
- [ğŸ“¦ Installation](#-installation)
  - [ğŸªŸ Windows Portable Package](#-windows-portable-package-recommended-for-windows)
  - [ğŸš€ Launch Scripts](#-launch-scripts) (Windows / Linux / macOS)
  - [Standard Installation](#standard-installation-all-platforms)
- [ğŸ“¥ Model Download](#-model-download)
- [ğŸš€ Usage](#-usage)
- [ğŸ“– Tutorial](#-tutorial)
- [ğŸ”¨ Train](#-train)
- [ğŸ—ï¸ Architecture](#ï¸-architecture)
- [ğŸ¦ Model Zoo](#-model-zoo)

## ğŸ“ Abstract
ğŸš€ We present ACE-Step v1.5, a highly efficient open-source music foundation model that brings commercial-grade generation to consumer hardware. On commonly used evaluation metrics, ACE-Step v1.5 achieves quality beyond most commercial music models while remaining extremely fastâ€”under 2 seconds per full song on an A100 and under 10 seconds on an RTX 3090. The model runs locally with less than 4GB of VRAM, and supports lightweight personalization: users can train a LoRA from just a few songs to capture their own style.

ğŸŒ‰ At its core lies a novel hybrid architecture where the Language Model (LM) functions as an omni-capable planner: it transforms simple user queries into comprehensive song blueprintsâ€”scaling from short loops to 10-minute compositionsâ€”while synthesizing metadata, lyrics, and captions via Chain-of-Thought to guide the Diffusion Transformer (DiT). âš¡ Uniquely, this alignment is achieved through intrinsic reinforcement learning relying solely on the model's internal mechanisms, thereby eliminating the biases inherent in external reward models or human preferences. ğŸšï¸

ğŸ”® Beyond standard synthesis, ACE-Step v1.5 unifies precise stylistic control with versatile editing capabilitiesâ€”such as cover generation, repainting, and vocal-to-BGM conversionâ€”while maintaining strict adherence to prompts across 50+ languages. This paves the way for powerful tools that seamlessly integrate into the creative workflows of music artists, producers, and content creators. ğŸ¸


## âœ¨ Features

<p align="center">
    <img src="./assets/application_map.png" width="100%" alt="ACE-Step Framework">
</p>

### âš¡ Performance
- âœ… **Ultra-Fast Generation** â€” Under 2s per full song on A100, under 10s on RTX 3090 (0.5s to 10s on A100 depending on think mode & diffusion steps)
- âœ… **Flexible Duration** â€” Supports 10 seconds to 10 minutes (600s) audio generation
- âœ… **Batch Generation** â€” Generate up to 8 songs simultaneously

### ğŸµ Generation Quality
- âœ… **Commercial-Grade Output** â€” Quality beyond most commercial music models (between Suno v4.5 and Suno v5)
- âœ… **Rich Style Support** â€” 1000+ instruments and styles with fine-grained timbre description
- âœ… **Multi-Language Lyrics** â€” Supports 50+ languages with lyrics prompt for structure & style control

### ğŸ›ï¸ Versatility & Control

| Feature | Description |
|---------|-------------|
| âœ… Reference Audio Input | Use reference audio to guide generation style |
| âœ… Cover Generation | Create covers from existing audio |
| âœ… Repaint & Edit | Selective local audio editing and regeneration |
| âœ… Track Separation | Separate audio into individual stems |
| âœ… Multi-Track Generation | Add layers like Suno Studio's "Add Layer" feature |
| âœ… Vocal2BGM | Auto-generate accompaniment for vocal tracks |
| âœ… Metadata Control | Control duration, BPM, key/scale, time signature |
| âœ… Simple Mode | Generate full songs from simple descriptions |
| âœ… Query Rewriting | Auto LM expansion of tags and lyrics |
| âœ… Audio Understanding | Extract BPM, key/scale, time signature & caption from audio |
| âœ… LRC Generation | Auto-generate lyric timestamps for generated music |
| âœ… LoRA Training | One-click annotation & training in Gradio. 8 songs, 1 hour on 3090 (12GB VRAM) |
| âœ… Quality Scoring | Automatic quality assessment for generated audio |

## Staying ahead
-----------------
Star ACE-Step on GitHub and be instantly notified of new releases
![](assets/star.gif)

## ğŸ“¦ Installation

> **Requirements:** Python 3.11, CUDA GPU recommended (works on CPU/MPS but slower)

### CPU Support (Limited)

ACE-Step can run on CPU for **inference only**, but performance will be significantly slower.

- CPU inference is supported for testing and experimentation.
- **Training (including LoRA) on CPU is not recommended** due to extremely long runtimes.
- For low-VRAM systems, DiT-only mode (LLM disabled) is supported.
- While CPU training may technically run, it is not practically usable due to extremely long runtimes; **for any practical training workflow, a CUDA-capable GPU or cloud GPU instance is required.**

If you do not have a CUDA GPU, consider:
- Using cloud GPU providers
- Running inference-only workflows
- Using DiT-only mode with `ACESTEP_INIT_LLM=false`

### AMD / ROCm GPUs

ACE-Step works with AMD GPUs via PyTorch ROCm builds.

**Important:** The `uv run acestep` workflow currently installs CUDA PyTorch wheels and may overwrite an existing ROCm setup. `uv run acestep` is optimized for CUDA environments and may override ROCm PyTorch installations.

#### Recommended workflow for AMD / ROCm users

1. Create and activate a virtual environment manually:

   ```bash
   python -m venv .venv
   source .venv/bin/activate
   ```

2. Install a ROCm-compatible PyTorch build:

   ```bash
   pip install torch --index-url https://download.pytorch.org/whl/rocm6.0
   ```

3. Install ACE-Step dependencies without using uv run:

   ```bash
   pip install -e .
   ```

4. Start the service directly:

   ```bash
   python -m acestep.acestep_v15_pipeline --port 7680
   ```

This avoids CUDA wheel replacement and has been confirmed to work on ROCm systems. On Windows, use `.venv\Scripts\activate` and the same steps.

#### Troubleshooting GPU Detection

If you see "No GPU detected, running on CPU" with an AMD GPU:

1. Run the GPU diagnostic tool:
   ```bash
   python scripts/check_gpu.py
   ```

2. For RDNA3 GPUs (RX 7000/9000 series), set `HSA_OVERRIDE_GFX_VERSION`:
   - RX 7900 XT/XTX, RX 9070 XT: `export HSA_OVERRIDE_GFX_VERSION=11.0.0` (Linux) or `set HSA_OVERRIDE_GFX_VERSION=11.0.0` (Windows)
   - RX 7800 XT, RX 7700 XT: `export HSA_OVERRIDE_GFX_VERSION=11.0.1`
   - RX 7600: `export HSA_OVERRIDE_GFX_VERSION=11.0.2`

3. On Windows, use `start_gradio_ui_rocm.bat` which automatically sets required environment variables.

4. Verify ROCm installation: `rocm-smi` should list your GPU.

### AMD / ROCM Linux Specific (cachy-os tested)
Date of the program this worked:
07.02.2026 - 10:40 am UTC +1

ACE-Step1.5 Rocm Manual for cachy-os and tested with RDNA4.
Look into docs\en\ACE-Step1.5-Rocm-Manual-Linux.md

### Linux + Python 3.11 note

Some Linux distributions (including Ubuntu) ship Python 3.11.0rc1, which is a **pre-release** build. This version is known to cause segmentation faults when using the vLLM backend due to C-extension incompatibilities.

**Recommendation:** Use a stable Python release (â‰¥ 3.11.12). On Ubuntu, this can be installed via the deadsnakes PPA.

If upgrading Python is not possible, use the PyTorch backend:

```bash
uv run acestep --backend pt
```

### ğŸªŸ Windows Portable Package (Recommended for Windows)

For Windows users, we provide a portable package with pre-installed dependencies:

1. Download and extract: [ACE-Step-1.5.7z](https://files.acemusic.ai/acemusic/win/ACE-Step-1.5.7z)
2. The package includes `python_embeded` with all dependencies pre-installed
3. **Requirements:** CUDA 12.8

The portable package auto-detects the environment:
1. `python_embeded\python.exe` (if exists)
2. `uv run acestep` (if uv is installed)
3. Auto-install uv via winget or PowerShell

#### ğŸ“¦ Portable Git Support

If you include a `PortableGit/` folder in your portable package, you can enable auto-update from GitHub before startup. Set `CHECK_UPDATE=true` in the launch scripts. When your modified files conflict with remote updates:
- Files are automatically backed up to `.update_backup_YYYYMMDD_HHMMSS/`
- Use `merge_config.bat` (Windows) or `merge_config.sh` (Linux/macOS) to compare and merge changes

---

### ğŸš€ Launch Scripts

We provide ready-to-use launch scripts for all platforms. These scripts handle environment detection, dependency installation, and application startup automatically. All scripts check for updates on startup by default (configurable).

#### ğŸªŸ Windows

| Script | Description | Usage |
|--------|-------------|-------|
| **start_gradio_ui.bat** | Launch Gradio Web UI (CUDA) | Double-click or run from terminal |
| **start_api_server.bat** | Launch REST API Server (CUDA) | Double-click or run from terminal |
| **start_gradio_ui_rocm.bat** | Launch Gradio Web UI (AMD ROCm) | For AMD RX 7000/6000 series GPUs |

```bash
# Launch Gradio Web UI (NVIDIA CUDA)
start_gradio_ui.bat

# Launch REST API Server (NVIDIA CUDA)
start_api_server.bat

# Launch Gradio Web UI (AMD ROCm)
start_gradio_ui_rocm.bat
```

> **ROCm users:** `start_gradio_ui_rocm.bat` auto-sets `HSA_OVERRIDE_GFX_VERSION`, `ACESTEP_LM_BACKEND=pt`, `MIOPEN_FIND_MODE=FAST` and other ROCm-specific environment variables. It uses a separate `venv_rocm` virtual environment to avoid CUDA/ROCm wheel conflicts. See [AMD / ROCm GPUs](#amd--rocm-gpus) for setup details.

#### ğŸ§ Linux

| Script | Description | Usage |
|--------|-------------|-------|
| **start_gradio_ui.sh** | Launch Gradio Web UI (CUDA) | `./start_gradio_ui.sh` |
| **start_api_server.sh** | Launch REST API Server (CUDA) | `./start_api_server.sh` |

```bash
# Make executable (first time only)
chmod +x start_gradio_ui.sh start_api_server.sh

# Launch Gradio Web UI
./start_gradio_ui.sh

# Launch REST API Server
./start_api_server.sh
```

> **Note:** Linux does not have a portable Git package like Windows. Git must be installed via your system package manager (`sudo apt install git`, `sudo yum install git`, `sudo pacman -S git`).

#### ğŸ macOS (Apple Silicon / MLX)

macOS scripts use the **MLX backend** for native Apple Silicon acceleration (M1/M2/M3/M4).

| Script | Description | Usage |
|--------|-------------|-------|
| **start_gradio_ui_macos.sh** | Launch Gradio Web UI (MLX) | `./start_gradio_ui_macos.sh` |
| **start_api_server_macos.sh** | Launch REST API Server (MLX) | `./start_api_server_macos.sh` |

```bash
# Make executable (first time only)
chmod +x start_gradio_ui_macos.sh start_api_server_macos.sh

# Launch Gradio Web UI with MLX backend
./start_gradio_ui_macos.sh

# Launch REST API Server with MLX backend
./start_api_server_macos.sh
```

The macOS scripts automatically:
- Set `ACESTEP_LM_BACKEND=mlx` and `--backend mlx` for native Apple Silicon acceleration
- Detect architecture and fall back to PyTorch backend on non-arm64 machines

> **Note:** macOS does not have a portable Git package. Install git via `xcode-select --install` or `brew install git`.

#### All Launch Scripts Support

- Startup update check (enabled by default, configurable)
- Auto environment detection (portable Python or uv)
- Auto install `uv` if needed
- Configurable download source (HuggingFace/ModelScope)
- Customizable models and parameters

#### ğŸ“ How to Modify Configuration

All configurable options are defined as variables at the top of each script. To customize, open the script with a text editor and modify the variable values.

**Example: Change UI language to Chinese and use the 1.7B LM model**

<table>
<tr><th>Windows (.bat)</th><th>Linux / macOS (.sh)</th></tr>
<tr><td>

Find these lines in `start_gradio_ui.bat`:
```batch
set LANGUAGE=en
set LM_MODEL_PATH=--lm_model_path acestep-5Hz-lm-0.6B
```
Change to:
```batch
set LANGUAGE=zh
set LM_MODEL_PATH=--lm_model_path acestep-5Hz-lm-1.7B
```

</td><td>

Find these lines in `start_gradio_ui.sh`:
```bash
LANGUAGE="en"
LM_MODEL_PATH="--lm_model_path acestep-5Hz-lm-0.6B"
```
Change to:
```bash
LANGUAGE="zh"
LM_MODEL_PATH="--lm_model_path acestep-5Hz-lm-1.7B"
```

</td></tr>
</table>

**Example: Disable startup update check**

<table>
<tr><th>Windows (.bat)</th><th>Linux / macOS (.sh)</th></tr>
<tr><td>

```batch
REM set CHECK_UPDATE=true
set CHECK_UPDATE=false
```

</td><td>

```bash
# CHECK_UPDATE="true"
CHECK_UPDATE="false"
```

</td></tr>
</table>

**Example: Enable a commented-out option** â€” remove the comment prefix (`REM` for .bat, `#` for .sh):

<table>
<tr><th>Windows (.bat)</th><th>Linux / macOS (.sh)</th></tr>
<tr><td>

Before:
```batch
REM set SHARE=--share
```
After:
```batch
set SHARE=--share
```

</td><td>

Before:
```bash
# SHARE="--share"
```
After:
```bash
SHARE="--share"
```

</td></tr>
</table>

**Common configurable options:**

| Option | Gradio UI | API Server | Description |
|--------|:---------:|:----------:|-------------|
| `LANGUAGE` | âœ… | â€” | UI language: `en`, `zh`, `he`, `ja` |
| `PORT` | âœ… | âœ… | Server port (default: 7860 / 8001) |
| `SERVER_NAME` / `HOST` | âœ… | âœ… | Bind address (`127.0.0.1` or `0.0.0.0`) |
| `CHECK_UPDATE` | âœ… | âœ… | Startup update check (`true` / `false`) |
| `CONFIG_PATH` | âœ… | â€” | DiT model (`acestep-v15-turbo`, etc.) |
| `LM_MODEL_PATH` | âœ… | âœ… | LM model (`acestep-5Hz-lm-0.6B` / `1.7B` / `4B`) |
| `DOWNLOAD_SOURCE` | âœ… | âœ… | Download source (`huggingface` / `modelscope`) |
| `SHARE` | âœ… | â€” | Create public Gradio link |
| `INIT_LLM` | âœ… | â€” | Force LLM on/off (`true` / `false` / `auto`) |
| `OFFLOAD_TO_CPU` | âœ… | â€” | CPU offload for low-VRAM GPUs |

#### ğŸ”„ Update & Maintenance Tools

| Script (Windows) | Script (Linux/macOS) | Purpose |
|-------------------|----------------------|---------|
| check_update.bat | check_update.sh | Check and update from GitHub |
| merge_config.bat | merge_config.sh | Merge backed-up configurations after update |
| install_uv.bat | install_uv.sh | Install uv package manager |
| quick_test.bat | quick_test.sh | Test environment setup |
| test_git_update.bat | test_git_update.sh | Test Git update functionality |
| test_env_detection.bat | test_env_detection.sh | Test environment auto-detection |

**Update Workflow:**

```bash
# Windows                          # Linux / macOS
check_update.bat                    ./check_update.sh
merge_config.bat                    ./merge_config.sh
```

**Environment Testing:**

```bash
# Windows                          # Linux / macOS
quick_test.bat                      ./quick_test.sh
```

**Update Features:**
- 10-second timeout protection (won't block startup if GitHub is unreachable)
- Smart conflict detection and automatic backup
- Automatic rollback on failure
- Preserves directory structure in backups

#### ğŸ› ï¸ Advanced Options

**Download Source:**
- `auto`: Auto-detect best source (checks Google accessibility)
- `huggingface`: Use HuggingFace Hub
- `modelscope`: Use ModelScope

---

### Standard Installation (All Platforms)

> **AMD / ROCm users:** `uv run acestep` is optimized for CUDA and may override ROCm PyTorch. Use the [AMD / ROCm workflow](#amd--rocm-gpus) above instead.

### 1. Install uv (Package Manager)

```bash
# macOS / Linux
curl -LsSf https://astral.sh/uv/install.sh | sh

# Windows (PowerShell)
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
```

### 2. Clone & Install

```bash
git clone https://github.com/ACE-Step/ACE-Step-1.5.git
cd ACE-Step-1.5
uv sync
```

### 3. Launch

#### ğŸ–¥ï¸ Gradio Web UI (Recommended)

**Using uv:**
```bash
uv run acestep
```

**Using Python directly:**

> **Note:** Make sure to activate your Python environment first:
> - **Windows portable package**: Use `python_embeded\python.exe` instead of `python`
> - **Conda environment**: Run `conda activate your_env_name` first
> - **venv**: Run `source venv/bin/activate` (Linux/Mac) or `venv\Scripts\activate` (Windows) first
> - **System Python**: Use `python` or `python3` directly

```bash
# Windows portable package
python_embeded\python.exe acestep\acestep_v15_pipeline.py

# Conda/venv/system Python
python acestep/acestep_v15_pipeline.py
```

Open http://localhost:7860 in your browser. Models will be downloaded automatically on first run.

#### ğŸŒ REST API Server

**Using uv:**
```bash
uv run acestep-api
```

**Using Python directly:**

> **Note:** Make sure to activate your Python environment first (see note above).

```bash
# Windows portable package
python_embeded\python.exe acestep\api_server.py

# Conda/venv/system Python
python acestep/api_server.py
```

API runs at http://localhost:8001. See [API Documentation](./docs/en/API.md) for endpoints.

### Command Line Options

**Gradio UI (`acestep`):**

| Option | Default | Description |
|--------|---------|-------------|
| `--port` | 7860 | Server port |
| `--server-name` | 127.0.0.1 | Server address (use `0.0.0.0` for network access) |
| `--share` | false | Create public Gradio link |
| `--language` | en | UI language: `en`, `zh`, `he`, `ja` |
| `--init_service` | false | Auto-initialize models on startup |
| `--init_llm` | auto | LLM initialization: `true` (force), `false` (disable), omit for auto |
| `--config_path` | auto | DiT model (e.g., `acestep-v15-turbo`, `acestep-v15-turbo-shift3`) |
| `--lm_model_path` | auto | LM model (e.g., `acestep-5Hz-lm-0.6B`, `acestep-5Hz-lm-1.7B`) |
| `--offload_to_cpu` | auto | CPU offload (auto-enabled if VRAM < 16GB) |
| `--download-source` | auto | Model download source: `auto`, `huggingface`, or `modelscope` |
| `--enable-api` | false | Enable REST API endpoints alongside Gradio UI |
| `--api-key` | none | API key for API endpoints authentication |
| `--auth-username` | none | Username for Gradio authentication |
| `--auth-password` | none | Password for Gradio authentication |

**Examples:**

> **Note for Python users:** Replace `python` with your environment's Python executable:
> - Windows portable package: `python_embeded\python.exe`
> - Conda: Activate environment first, then use `python`
> - venv: Activate environment first, then use `python`
> - System: Use `python` or `python3`

```bash
# Public access with Chinese UI
uv run acestep --server-name 0.0.0.0 --share --language zh
# Or using Python directly:
python acestep/acestep_v15_pipeline.py --server-name 0.0.0.0 --share --language zh

# Pre-initialize models on startup
uv run acestep --init_service true --config_path acestep-v15-turbo
# Or using Python directly:
python acestep/acestep_v15_pipeline.py --init_service true --config_path acestep-v15-turbo

# Enable API endpoints with authentication
uv run acestep --enable-api --api-key sk-your-secret-key --port 8001
# Or using Python directly:
python acestep/acestep_v15_pipeline.py --enable-api --api-key sk-your-secret-key --port 8001

# Enable both Gradio auth and API auth
uv run acestep --enable-api --api-key sk-123456 --auth-username admin --auth-password password
# Or using Python directly:
python acestep/acestep_v15_pipeline.py --enable-api --api-key sk-123456 --auth-username admin --auth-password password

# Use ModelScope as download source
uv run acestep --download-source modelscope
# Or using Python directly:
python acestep/acestep_v15_pipeline.py --download-source modelscope

# Use HuggingFace Hub as download source
uv run acestep --download-source huggingface
# Or using Python directly:
python acestep/acestep_v15_pipeline.py --download-source huggingface
```

### Environment Variables (.env)

For `uv` or Python users, you can configure ACE-Step using environment variables in a `.env` file:

```bash
# Copy the example file
cp .env.example .env

# Edit .env with your settings
```

**Key environment variables:**

| Variable | Values | Description |
|----------|--------|-------------|
| `ACESTEP_INIT_LLM` | (empty), `true`, `false` | LLM initialization mode |
| `ACESTEP_CONFIG_PATH` | model name | DiT model path |
| `ACESTEP_LM_MODEL_PATH` | model name | LM model path |
| `ACESTEP_DOWNLOAD_SOURCE` | `auto`, `huggingface`, `modelscope` | Download source |
| `ACESTEP_API_KEY` | string | API authentication key |

**LLM Initialization (`ACESTEP_INIT_LLM`):**

Processing flow: `GPU Detection (full) â†’ ACESTEP_INIT_LLM Override â†’ Model Loading`

GPU optimizations (offload, quantization, batch limits) are **always applied**. The override only controls whether to attempt LLM loading.

| Value | Behavior |
|-------|----------|
| `auto` (or empty) | Use GPU auto-detection result (recommended) |
| `true` / `1` / `yes` | Force enable LLM after GPU detection (may cause OOM) |
| `false` / `0` / `no` | Force disable for pure DiT mode, faster generation |

**Example `.env` for different scenarios:**

```bash
# Auto mode (recommended) - let GPU detection decide
ACESTEP_INIT_LLM=auto

# Force enable on low VRAM GPU (GPU optimizations still applied)
ACESTEP_INIT_LLM=true
ACESTEP_LM_MODEL_PATH=acestep-5Hz-lm-0.6B

# Force disable LLM for faster generation
ACESTEP_INIT_LLM=false
```

### Development

```bash
# Add dependencies
uv add package-name
uv add --dev package-name

# Update all dependencies
uv sync --upgrade
```

## ğŸ® Other GPU Support

### Intel GPU
Currently, we support Intel GPUs.
- **Tested Device**: Windows laptop with Ultra 9 285H integrated graphics.
- **Settings**:
  - `offload` is disabled by default.
  - `compile` and `quantization` are enabled by default.
- **Capabilities**: LLM inference is supported (tested with `acestep-5Hz-lm-0.6B`).
  - *Note*: LLM inference speed might decrease when generating audio longer than 2 minutes.
  - *Note*: `nanovllm` acceleration for LLM inference is currently NOT supported on Intel GPUs.
- **Test Environment**: PyTorch 2.8.0 from [Intel Extension for PyTorch](https://pytorch-extension.intel.com/?request=platform).
- **Intel Discrete GPUs**: Expected to work, but not tested yet as the developer does not have available devices. Waiting for community feedback.

## ğŸ“¥ Model Download

Models are automatically downloaded from [HuggingFace](https://huggingface.co/ACE-Step/Ace-Step1.5) or [ModelScope](https://modelscope.cn/organization/ACE-Step) on first run. You can also manually download models using the CLI or `huggingface-cli`.

### Automatic Download

When you run `acestep` or `acestep-api`, the system will:
1. Check if the required models exist in `./checkpoints`
2. If not found, automatically download them using the configured source (or auto-detect)

### Manual Download with CLI

> **Note for Python users:** Replace `python` with your environment's Python executable (see note in Launch section above).

**Using uv:**
```bash
# Download main model (includes everything needed to run)
uv run acestep-download

# Download all available models (including optional variants)
uv run acestep-download --all

# Download from ModelScope
uv run acestep-download --download-source modelscope

# Download from HuggingFace Hub
uv run acestep-download --download-source huggingface

# Download a specific model
uv run acestep-download --model acestep-v15-sft

# List all available models
uv run acestep-download --list

# Download to a custom directory
uv run acestep-download --dir /path/to/checkpoints
```

**Using Python directly:**
```bash
# Download main model (includes everything needed to run)
python -m acestep.model_downloader

# Download all available models (including optional variants)
python -m acestep.model_downloader --all

# Download from ModelScope
python -m acestep.model_downloader --download-source modelscope

# Download from HuggingFace Hub
python -m acestep.model_downloader --download-source huggingface

# Download a specific model
python -m acestep.model_downloader --model acestep-v15-sft

# List all available models
python -m acestep.model_downloader --list

# Download to a custom directory
python -m acestep.model_downloader --dir /path/to/checkpoints
```

### Manual Download with huggingface-cli

You can also use `huggingface-cli` directly:

```bash
# Download main model (includes vae, Qwen3-Embedding-0.6B, acestep-v15-turbo, acestep-5Hz-lm-1.7B)
huggingface-cli download ACE-Step/Ace-Step1.5 --local-dir ./checkpoints

# Download optional LM models
huggingface-cli download ACE-Step/acestep-5Hz-lm-0.6B --local-dir ./checkpoints/acestep-5Hz-lm-0.6B
huggingface-cli download ACE-Step/acestep-5Hz-lm-4B --local-dir ./checkpoints/acestep-5Hz-lm-4B

# Download optional DiT models
huggingface-cli download ACE-Step/acestep-v15-base --local-dir ./checkpoints/acestep-v15-base
huggingface-cli download ACE-Step/acestep-v15-sft --local-dir ./checkpoints/acestep-v15-sft
huggingface-cli download ACE-Step/acestep-v15-turbo-shift1 --local-dir ./checkpoints/acestep-v15-turbo-shift1
huggingface-cli download ACE-Step/acestep-v15-turbo-shift3 --local-dir ./checkpoints/acestep-v15-turbo-shift3
huggingface-cli download ACE-Step/acestep-v15-turbo-continuous --local-dir ./checkpoints/acestep-v15-turbo-continuous
```

### Available Models

| Model | HuggingFace Repo | Description |
|-------|------------------|-------------|
| **Main** | [ACE-Step/Ace-Step1.5](https://huggingface.co/ACE-Step/Ace-Step1.5) | Core components: vae, Qwen3-Embedding-0.6B, acestep-v15-turbo, acestep-5Hz-lm-1.7B |
| acestep-5Hz-lm-0.6B | [ACE-Step/acestep-5Hz-lm-0.6B](https://huggingface.co/ACE-Step/acestep-5Hz-lm-0.6B) | Lightweight LM model (0.6B params) |
| acestep-5Hz-lm-4B | [ACE-Step/acestep-5Hz-lm-4B](https://huggingface.co/ACE-Step/acestep-5Hz-lm-4B) | Large LM model (4B params) |
| acestep-v15-base | [ACE-Step/acestep-v15-base](https://huggingface.co/ACE-Step/acestep-v15-base) | Base DiT model |
| acestep-v15-sft | [ACE-Step/acestep-v15-sft](https://huggingface.co/ACE-Step/acestep-v15-sft) | SFT DiT model |
| acestep-v15-turbo-shift1 | [ACE-Step/acestep-v15-turbo-shift1](https://huggingface.co/ACE-Step/acestep-v15-turbo-shift1) | Turbo DiT with shift1 |
| acestep-v15-turbo-shift3 | [ACE-Step/acestep-v15-turbo-shift3](https://huggingface.co/ACE-Step/acestep-v15-turbo-shift3) | Turbo DiT with shift3 |
| acestep-v15-turbo-continuous | [ACE-Step/acestep-v15-turbo-continuous](https://huggingface.co/ACE-Step/acestep-v15-turbo-continuous) | Turbo DiT with continuous shift (1-5) |

### ğŸ’¡ Which Model Should I Choose?

ACE-Step automatically adapts to your GPU's VRAM. Here's a quick guide:

| Your GPU VRAM | Recommended LM Model | Notes |
|---------------|---------------------|-------|
| **â‰¤6GB** | None (DiT only) | LM disabled by default to save memory |
| **6-12GB** | `acestep-5Hz-lm-0.6B` | Lightweight, good balance |
| **12-16GB** | `acestep-5Hz-lm-1.7B` | Better quality |
| **â‰¥16GB** | `acestep-5Hz-lm-4B` | Best quality and audio understanding |

> ğŸ“– **For detailed GPU compatibility information** (duration limits, batch sizes, memory optimization), see GPU Compatibility Guide: [English](./docs/en/GPU_COMPATIBILITY.md) | [ä¸­æ–‡](./docs/zh/GPU_COMPATIBILITY.md) | [æ—¥æœ¬èª](./docs/ja/GPU_COMPATIBILITY.md)


## ğŸš€ Usage

We provide multiple ways to use ACE-Step:

| Method | Description | Documentation |
|--------|-------------|---------------|
| ğŸ–¥ï¸ **Gradio Web UI** | Interactive web interface for music generation | [Gradio Guide](./docs/en/GRADIO_GUIDE.md) |
| ğŸšï¸ **Studio UI (Experimental)** | Optional HTML frontend for REST API (DAW-like) | [Studio UI](./docs/en/studio.md) |
| ğŸ **Python API** | Programmatic access for integration | [Inference API](./docs/en/INFERENCE.md) |
| ğŸŒ **REST API** | HTTP-based async API for services | [REST API](./docs/en/API.md) |
| âŒ¨ï¸ **CLI** | Interactive wizard and configuration for commandline interface | [CLI Guide](./docs/en/CLI.md) |

**ğŸ“š Documentation available in:** [English](./docs/en/) | [ä¸­æ–‡](./docs/zh/) | [æ—¥æœ¬èª](./docs/ja/)

### Experimental Studio UI

An optional, frontend-only HTML Studio UI is available for users who prefer a more structured interface. It uses the same REST API and does not change backend behavior. Start the API server, then open `ui/studio.html` in a browser and point it at your API URL. See [Studio UI](./docs/en/studio.md).

## ğŸ“– Tutorial

**ğŸ¯ Must Read:** Comprehensive guide to ACE-Step 1.5's design philosophy and usage methods.

| Language | Link |
|----------|------|
| ğŸ‡ºğŸ‡¸ English | [English Tutorial](./docs/en/Tutorial.md) |
| ğŸ‡¨ğŸ‡³ ä¸­æ–‡ | [ä¸­æ–‡æ•™ç¨‹](./docs/zh/Tutorial.md) |
| ğŸ‡¯ğŸ‡µ æ—¥æœ¬èª | [æ—¥æœ¬èªãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](./docs/ja/Tutorial.md) |

This tutorial covers:
- Mental models and design philosophy
- Model architecture and selection
- Input control (text and audio)
- Inference hyperparameters
- Random factors and optimization strategies

## ğŸ”¨ Train

See the **LoRA Training** tab in Gradio UI for one-click training, or check [Gradio Guide - LoRA Training](./docs/en/GRADIO_GUIDE.md#lora-training) for details.

## ğŸ—ï¸ Architecture

<p align="center">
    <img src="./assets/ACE-Step_framework.png" width="100%" alt="ACE-Step Framework">
</p>

## ğŸ¦ Model Zoo

<p align="center">
    <img src="./assets/model_zoo.png" width="100%" alt="Model Zoo">
</p>

### DiT Models

| DiT Model | Pre-Training | SFT | RL | CFG | Step | Refer audio | Text2Music | Cover | Repaint | Extract | Lego | Complete | Quality | Diversity | Fine-Tunability | Hugging Face |
|-----------|:------------:|:---:|:--:|:---:|:----:|:-----------:|:----------:|:-----:|:-------:|:-------:|:----:|:--------:|:-------:|:---------:|:---------------:|--------------|
| `acestep-v15-base` | âœ… | âŒ | âŒ | âœ… | 50 | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | Medium | High | Easy | [Link](https://huggingface.co/ACE-Step/acestep-v15-base) |
| `acestep-v15-sft` | âœ… | âœ… | âŒ | âœ… | 50 | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âŒ | High | Medium | Easy | [Link](https://huggingface.co/ACE-Step/acestep-v15-sft) |
| `acestep-v15-turbo` | âœ… | âœ… | âŒ | âŒ | 8 | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âŒ | Very High | Medium | Medium | [Link](https://huggingface.co/ACE-Step/Ace-Step1.5) |
| `acestep-v15-turbo-rl` | âœ… | âœ… | âœ… | âŒ | 8 | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âŒ | Very High | Medium | Medium | To be released |

### LM Models

| LM Model | Pretrain from | Pre-Training | SFT | RL | CoT metas | Query rewrite | Audio Understanding | Composition Capability | Copy Melody | Hugging Face |
|----------|---------------|:------------:|:---:|:--:|:---------:|:-------------:|:-------------------:|:----------------------:|:-----------:|--------------|
| `acestep-5Hz-lm-0.6B` | Qwen3-0.6B | âœ… | âœ… | âœ… | âœ… | âœ… | Medium | Medium | Weak | âœ… |
| `acestep-5Hz-lm-1.7B` | Qwen3-1.7B | âœ… | âœ… | âœ… | âœ… | âœ… | Medium | Medium | Medium | âœ… |
| `acestep-5Hz-lm-4B` | Qwen3-4B | âœ… | âœ… | âœ… | âœ… | âœ… | Strong | Strong | Strong | âœ… |

## ğŸ“œ License & Disclaimer

This project is licensed under [MIT](./LICENSE)

ACE-Step enables original music generation across diverse genres, with applications in creative production, education, and entertainment. While designed to support positive and artistic use cases, we acknowledge potential risks such as unintentional copyright infringement due to stylistic similarity, inappropriate blending of cultural elements, and misuse for generating harmful content. To ensure responsible use, we encourage users to verify the originality of generated works, clearly disclose AI involvement, and obtain appropriate permissions when adapting protected styles or materials. By using ACE-Step, you agree to uphold these principles and respect artistic integrity, cultural diversity, and legal compliance. The authors are not responsible for any misuse of the model, including but not limited to copyright violations, cultural insensitivity, or the generation of harmful content.

ğŸ”” Important Notice  
The only official website for the ACE-Step project is our GitHub Pages site.    
 We do not operate any other websites.  
ğŸš« Fake domains include but are not limited to:
ac\*\*p.com, a\*\*p.org, a\*\*\*c.org  
âš ï¸ Please be cautious. Do not visit, trust, or make payments on any of those sites.

## ğŸ™ Acknowledgements

This project is co-led by ACE Studio and StepFun.


## ğŸ“– Citation

If you find this project useful for your research, please consider citing:

```BibTeX
@misc{gong2026acestep,
	title={ACE-Step 1.5: Pushing the Boundaries of Open-Source Music Generation},
	author={Junmin Gong, Yulin Song, Wenxiao Zhao, Sen Wang, Shengyuan Xu, Jing Guo}, 
	howpublished={\url{https://github.com/ace-step/ACE-Step-1.5}},
	year={2026},
	note={GitHub repository}
}
```
